---
tags:
  - AI
  - numpy
---
Нейронные сети представляют собой мощный инструмент для решения задач машинного обучения, имитируя работу человеческого мозга. В этом конспекте мы рассмотрим основные концепции нейронных сетей, их реализацию на Python, а также математические функции и матрицы, используемые в процессе.

### **** Основные концепции нейронных сетей

- **Нейрон**: Основная единица нейронной сети, которая принимает входные данные, применяет веса и функцию активации для генерации выхода.
  
- **Слои**: Нейроны организованы в слои:
  - **Входной слой**: Получает входные данные.
  - **Скрытые слои**: Обрабатывают данные.
  - **Выходной слой**: Предоставляет результат.

- **Обучение**: Процесс настройки весов нейронов на основе обучающего набора данных. Обычно используется метод обратного распространения ошибки.

### **** Математические функции и матрицы

1. **Функция активации**: Применяется для преобразования взвешенной суммы входов в выход. Наиболее распространенные функции:
   - Сигмоида: 
     $$
     \sigma(x) = \frac{1}{1 + e^{-x}}
     $$
   - Тангенс гиперболический:
     $$
     \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
     $$

2. **Матрицы весов**: Используются для хранения весов между нейронами. Например, если у нас есть 3 входа и 1 выход, матрица весов будет иметь размерность $3 \times 1$.

3. **Обратное распространение ошибки**: Алгоритм для корректировки весов на основе разницы между предсказанным и фактическим выходом.

### **** Пример реализации нейронной сети на Python

Ниже представлен простой пример реализации нейронной сети с использованием библиотеки NumPy.

```python
import numpy as np

# Функция активации (сигмоида)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Производная сигмоида
def sigmoid_derivative(x):
    return x * (1 - x)

# Обучающий набор данных
X = np.array([[0, 0, 1],
              [1, 1, 1],
              [1, 0, 1],
              [0, 1, 1]])

y = np.array([[0], [1], [1], [0]])

# Инициализация весов
np.random.seed(1)
weights = np.random.rand(3, 1)

# Обучение сети
for epoch in range(10000):
    # Прямое распространение
    input_layer = X
    outputs = sigmoid(np.dot(input_layer, weights))
    
    # Ошибка
    error = y - outputs
    
    # Корректировка весов
    adjustments = error * sigmoid_derivative(outputs)
    weights += np.dot(input_layer.T, adjustments)

# Тестирование сети
print("Выход после обучения:")
print(outputs)
```

### **** Пояснение кода

- **Импорт библиотек**: Мы используем NumPy для работы с массивами и матрицами.
  
- **Функция активации**: Сигмоида используется для нормализации выходных значений.

- **Обучающий набор данных**: Входные данные представлены в виде матрицы $X$, а ожидаемые выходы — в виде вектора $y$.

- **Инициализация весов**: Случайные веса генерируются с помощью `np.random.rand`.

- **Процесс обучения**:
  - Выполняется прямое распространение (forward propagation) для получения выходных данных.
  - Вычисляется ошибка между фактическим и предсказанным значением.
  - Весы корректируются на основе производной функции активации и ошибки.


Citations:
[1] https://blog.skillfactory.ru/kak-postroit-svoyu-pervuyu-nejronnuyu-set-napisav-9-strochek-na-python/
[2] https://timeweb.cloud/tutorials/machine-learning/kak-napisat-prostuyu-nejroset-na-python
[3] https://habr.com/ru/articles/755096/
[4] https://proglib.io/p/pishem-neyroset-na-python-s-nulya-2020-10-07
[5] https://habr.com/ru/articles/271563/
[6] https://gb.ru/blog/nejronnye-seti-python/
[7] https://habr.com/ru/articles/725668/
[8] https://sky.pro/wiki/python/primery-programm-nejronnyh-setej/
[9] https://kpfu.ru/portal/docs/F_1458204831/Nejronnye.seti.na.Python.pdf
[10] https://www.youtube.com/watch?v=bXGBeRzM87g